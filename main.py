import torch
import torch.nn as nn
from torch.nn import functional as F
from torch.utils.data import Dataset, DataLoader
import json
import tiktoken
import os
import urllib.request
import time

# ==========================================
# 1. –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# ==========================================
# –£–≤–µ–ª–∏—á–∏–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, —Ç–∞–∫ –∫–∞–∫ —Ç–µ–ø–µ—Ä—å –º—ã —Å–µ—Ä—å–µ–∑–Ω—ã
BATCH_SIZE = 8       # –ï—Å–ª–∏ –≤—ã–ª–µ—Ç–∞–µ—Ç –æ—à–∏–±–∫–∞ –ø–∞–º—è—Ç–∏ (OOM), —É–º–µ–Ω—å—à–∏—Ç–µ –¥–æ 4 –∏–ª–∏ 2
BLOCK_SIZE = 128     # –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
LEARNING_RATE = 3e-4
EMBED_DIM = 384      # –ß—É—Ç—å –±–æ–ª—å—à–µ "–º–æ–∑–≥–æ–≤" (–±—ã–ª–æ 256)
NUM_HEADS = 6        # 384 / 6 = 64 —Ä–∞–∑–º–µ—Ä –≥–æ–ª–æ–≤—ã
NUM_LAYERS = 6       # –ë–æ–ª—å—à–µ —Å–ª–æ–µ–≤ –¥–ª—è –≥–ª—É–±–∏–Ω—ã
DROPOUT = 0.1
EPOCHS = 3           # –ü—Ä–æ–π–¥–µ–º –ø–æ –≤—Å–µ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É 3 —Ä–∞–∑–∞

# –ù–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
MODEL_PATH = "my_alpaca_gpt.pt"

# --- –õ–û–ì–ò–ö–ê –í–´–ë–û–†–ê –£–°–¢–†–û–ô–°–¢–í–ê (CUDA / ROCm / MPS / CPU) ---
def get_device():
    # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º CUDA (NVIDIA) –∏–ª–∏ ROCm (AMD)
    # PyTorch –¥–ª—è ROCm –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å 'cuda', –ø–æ—ç—Ç–æ–º—É is_available() –≤–µ—Ä–Ω–µ—Ç True
    if torch.cuda.is_available():
        # –ü—Ä–æ–≤–µ—Ä–∏–º, —ç—Ç–æ AMD –∏–ª–∏ Nvidia
        if torch.version.hip:
            print(f"‚úÖ –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: AMD GPU (ROCm) | {torch.cuda.get_device_name(0)}")
        else:
            print(f"‚úÖ –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: NVIDIA GPU (CUDA) | {torch.cuda.get_device_name(0)}")
        return 'cuda'
    
    # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º Apple Metal (Mac M1/M2/M3)
    elif torch.backends.mps.is_available():
        print("‚úÖ –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: Apple Silicon (MPS/Metal)")
        return 'mps'
    
    # 3. Fallback –Ω–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
    else:
        print("‚ö†Ô∏è –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: CPU (–í–Ω–∏–º–∞–Ω–∏–µ: –æ–±—É—á–µ–Ω–∏–µ –±—É–¥–µ—Ç –æ—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω—ã–º)")
        return 'cpu'

DEVICE = get_device()
# ---------------------------------------------------

# ==========================================
# 2. –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–¢–ê–°–ï–¢–ê
# ==========================================
class AlpacaDataset(Dataset):
    def __init__(self, json_file, tokenizer, max_length=BLOCK_SIZE):
        if not os.path.exists(json_file):
            print("‚è≥ –°–∫–∞—á–∏–≤–∞—é alpaca_data.json...")
            url = "https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json"
            urllib.request.urlretrieve(url, json_file)
        
        print("‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ JSON...")
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.tokenizer = tokenizer
        self.samples = []
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –í–ï–°–¨ –¥–∞—Ç–∞—Å–µ—Ç (52k –ø—Ä–∏–º–µ—Ä–æ–≤)
        print(f"–í—Å–µ–≥–æ –¥–∏–∞–ª–æ–≥–æ–≤ –≤ —Ñ–∞–π–ª–µ: {len(data)}. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤...")
        
        for item in data:
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É –æ–±—É—á–µ–Ω–∏—è
            # –î–æ–±–∞–≤–ª—è–µ–º —è–≤–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –º–æ–¥–µ–ª–∏
            text = f"User: {item['instruction']} {item['input']}\nBot: {item['output']}<|endoftext|>"
            self.samples.append(text)

        print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –≥–æ—Ç–æ–≤. –ü—Ä–∏–º–µ—Ä–æ–≤: {len(self.samples)}")
        self.max_length = max_length

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        text = self.samples[idx]
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        tokens = self.tokenizer.encode(text, allowed_special={'<|endoftext|>'})
        
        # –û–±—Ä–µ–∑–∫–∞ –∏–ª–∏ –ø–∞–¥–¥–∏–Ω–≥
        if len(tokens) > self.max_length:
            tokens = tokens[:self.max_length]
        else:
            # 50256 - —ç—Ç–æ —Ç–æ–∫–µ–Ω <|endoftext|> –≤ GPT-2
            tokens = tokens + [50256] * (self.max_length - len(tokens))
            
        data = torch.tensor(tokens, dtype=torch.long)
        x = data[:-1]
        y = data[1:]
        return x, y

# ==========================================
# 3. –ê–†–•–ò–¢–ï–ö–¢–£–†–ê –ú–û–î–ï–õ–ò (GPT)
# ==========================================
class Head(nn.Module):
    def __init__(self, head_size):
        super().__init__()
        self.key = nn.Linear(EMBED_DIM, head_size, bias=False)
        self.query = nn.Linear(EMBED_DIM, head_size, bias=False)
        self.value = nn.Linear(EMBED_DIM, head_size, bias=False)
        self.register_buffer('tril', torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)))
        self.dropout = nn.Dropout(DROPOUT)

    def forward(self, x):
        B, T, C = x.shape
        k = self.key(x)
        q = self.query(x)
        # Compute attention scores
        wei = q @ k.transpose(-2, -1) * C**-0.5
        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))
        wei = F.softmax(wei, dim=-1)
        wei = self.dropout(wei)
        v = self.value(x)
        out = wei @ v
        return out

class MultiHeadAttention(nn.Module):
    def __init__(self, num_heads, head_size):
        super().__init__()
        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])
        self.proj = nn.Linear(EMBED_DIM, EMBED_DIM)
        self.dropout = nn.Dropout(DROPOUT)

    def forward(self, x):
        out = torch.cat([h(x) for h in self.heads], dim=-1)
        out = self.dropout(self.proj(out))
        return out

class FeedFoward(nn.Module):
    def __init__(self, n_embd):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(n_embd, 4 * n_embd),
            nn.ReLU(),
            nn.Linear(4 * n_embd, n_embd),
            nn.Dropout(DROPOUT),
        )

    def forward(self, x):
        return self.net(x)

class Block(nn.Module):
    def __init__(self, n_embd, n_head):
        super().__init__()
        head_size = n_embd // n_head
        self.sa = MultiHeadAttention(n_head, head_size)
        self.ffwd = FeedFoward(n_embd)
        self.ln1 = nn.LayerNorm(n_embd)
        self.ln2 = nn.LayerNorm(n_embd)

    def forward(self, x):
        x = x + self.sa(self.ln1(x))
        x = x + self.ffwd(self.ln2(x))
        return x

class BabyGPT(nn.Module):
    def __init__(self, vocab_size):
        super().__init__()
        self.token_embedding_table = nn.Embedding(vocab_size, EMBED_DIM)
        self.position_embedding_table = nn.Embedding(BLOCK_SIZE, EMBED_DIM)
        self.blocks = nn.Sequential(*[Block(EMBED_DIM, NUM_HEADS) for _ in range(NUM_LAYERS)])
        self.ln_f = nn.LayerNorm(EMBED_DIM)
        self.lm_head = nn.Linear(EMBED_DIM, vocab_size)

    def forward(self, idx, targets=None):
        B, T = idx.shape
        # device=idx.device –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ–∑–¥–∞—é—Ç—Å—è —Ç–∞–º –∂–µ, –≥–¥–µ –¥–∞–Ω–Ω—ã–µ (MPS/CUDA)
        tok_emb = self.token_embedding_table(idx)
        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))
        x = tok_emb + pos_emb
        x = self.blocks(x)
        x = self.ln_f(x)
        logits = self.lm_head(x)

        loss = None
        if targets is not None:
            B, T, C = logits.shape
            logits = logits.view(B*T, C)
            targets = targets.view(B*T)
            loss = F.cross_entropy(logits, targets)

        return logits, loss

# ==========================================
# 4. –ó–ê–ü–£–°–ö
# ==========================================
if __name__ == '__main__':
    torch.manual_seed(1337)
    
    # 1. –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
    try:
        tokenizer = tiktoken.get_encoding("gpt2")
    except:
        print("–û—à–∏–±–∫–∞: –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω tiktoken. –í—ã–ø–æ–ª–Ω–∏—Ç–µ pip install tiktoken")
        exit()
    
    # 2. –î–∞—Ç–∞—Å–µ—Ç –∏ Dataloader
    dataset = AlpacaDataset('alpaca_data.json', tokenizer)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

    # 3. –ú–æ–¥–µ–ª—å
    print(f"–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ (Layers: {NUM_LAYERS}, Heads: {NUM_HEADS}, Emb: {EMBED_DIM})...")
    model = BabyGPT(vocab_size=50304) # 50304 - —Å—Ç–∞–Ω–¥–∞—Ä—Ç GPT-2 (–∫—Ä–∞—Å–∏–≤–æ –¥–µ–ª–∏—Ç—Å—è)
    
    # –ü–µ—Ä–µ–Ω–æ—Å –º–æ–¥–µ–ª–∏ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    model = model.to(DEVICE)
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
    
    num_params = sum(p.numel() for p in model.parameters())
    print(f"üî• –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞. –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {num_params/1e6:.2f} M")
    print(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ {DEVICE} | –≠–ø–æ—Ö: {EPOCHS}")

    # --- –¶–ò–ö–õ –û–ë–£–ß–ï–ù–ò–Ø ---
    model.train()
    start_time = time.time()
    
    for epoch in range(EPOCHS):
        for i, (xb, yb) in enumerate(dataloader):
            # –ü–µ—Ä–µ–Ω–æ—Å –±–∞—Ç—á–∞ –Ω–∞ GPU/MPS
            xb, yb = xb.to(DEVICE), yb.to(DEVICE)
            
            # Forward
            logits, loss = model(xb, yb)
            
            # Backward
            optimizer.zero_grad(set_to_none=True)
            loss.backward()
            optimizer.step()
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
            if i % 50 == 0:
                elapsed = time.time() - start_time
                print(f"Epoch {epoch+1}/{EPOCHS} | Step {i} | Loss: {loss.item():.4f} | Time: {elapsed:.1f}s")
                # –ú–æ–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç—ã –∫–∞–∂–¥—ã–µ N —à–∞–≥–æ–≤, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ

    print("üèÅ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    
    # --- –°–û–•–†–ê–ù–ï–ù–ò–ï ---
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω—è—é –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏ –≤ {MODEL_PATH}...")
    torch.save(model.state_dict(), MODEL_PATH)

    # ==========================================
    # 5. –¢–ï–°–¢ –ì–ï–ù–ï–†–ê–¶–ò–ò (INFERENCE)
    # ==========================================
    print("\n--- ü§ñ –¢–ï–°–¢ –ß–ê–¢-–ë–û–¢–ê ---")
    model.eval()
    
    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è —á–∏—Å—Ç–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    def generate_response(prompt, max_tokens=100):
        full_prompt = f"User: {prompt}\nBot:"
        input_ids = tokenizer.encode(full_prompt)
        x = torch.tensor([input_ids], dtype=torch.long, device=DEVICE)
        
        # –°–ø–∏—Å–æ–∫ –¥–ª—è —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        generated = []
        
        with torch.no_grad():
            for _ in range(max_tokens):
                # –û–±—Ä–µ–∑–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç, –µ—Å–ª–∏ –æ–Ω —Å—Ç–∞–ª —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–º
                idx_cond = x[:, -BLOCK_SIZE:]
                
                # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                logits, _ = model(idx_cond)
                logits = logits[:, -1, :]
                
                # Sampling (–≤—ã–±–æ—Ä —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é)
                probs = F.softmax(logits, dim=-1)
                idx_next = torch.multinomial(probs, num_samples=1)
                token_id = idx_next.item()
                
                # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å —Ä–µ—à–∏–ª–∞ –∑–∞–∫–æ–Ω—á–∏—Ç—å —Ñ—Ä–∞–∑—É (—Ç–æ–∫–µ–Ω <|endoftext|>)
                if token_id == 50256:
                    break
                
                generated.append(token_id)
                # –î–æ–±–∞–≤–ª—è–µ–º –∫ –≤—Ö–æ–¥—É –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —à–∞–≥–∞
                x = torch.cat((x, idx_next), dim=1)
        
        return tokenizer.decode(generated)

    # –ü—Ä–∏–º–µ—Ä—ã –≤–æ–ø—Ä–æ—Å–æ–≤
    questions = [
        "Hello, how are you?",
        "What is Python?",
        "Tell me a story about a cat."
    ]

    for q in questions:
        print(f"\nUser: {q}")
        ans = generate_response(q)
        print(f"Bot: {ans}")
